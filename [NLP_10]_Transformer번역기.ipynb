{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[NLP-10] Transformer번역기.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d0e231d2558460b87e2b216be8d0407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18696c9ff70149f9967fe591f8adf8bd",
              "IPY_MODEL_97a09b05ac354b3683786234883c8851",
              "IPY_MODEL_7420dcf1374f4f7b9b789a7cc79383a9"
            ],
            "layout": "IPY_MODEL_313bb82aff944914beb94723e3710ddd"
          }
        },
        "18696c9ff70149f9967fe591f8adf8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_496194799097486ba212fd4e22addaa2",
            "placeholder": "​",
            "style": "IPY_MODEL_0699a8e0144e41e081a1d6a0cf558e75",
            "value": "100%"
          }
        },
        "97a09b05ac354b3683786234883c8851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b7cd9deb21e41af895901b516b655b4",
            "max": 78968,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66611682c8e247bcb2385434058b151a",
            "value": 78968
          }
        },
        "7420dcf1374f4f7b9b789a7cc79383a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69b4897748df46ebb0b736d4d594a944",
            "placeholder": "​",
            "style": "IPY_MODEL_9dceb35ec7e34779a4975deddfba5fcc",
            "value": " 78968/78968 [00:04&lt;00:00, 20580.23it/s]"
          }
        },
        "313bb82aff944914beb94723e3710ddd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "496194799097486ba212fd4e22addaa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0699a8e0144e41e081a1d6a0cf558e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b7cd9deb21e41af895901b516b655b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66611682c8e247bcb2385434058b151a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69b4897748df46ebb0b736d4d594a944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dceb35ec7e34779a4975deddfba5fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-sudJcBIPAR",
        "outputId": "f68a4384-b2ba-4a4e-83a0-8fde718499df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        " \n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager.findfont(font)\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "\n",
        "import seaborn # Attention 시각화를 위해 필요!\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3FBSDzNIbu-",
        "outputId": "89bf0c3e-3667-4de9-c2e1-e070f3efe0c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "UrfFRLj7I1hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, int(i) / d_model)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "    return sinusoid_table\n",
        "\n"
      ],
      "metadata": {
        "id": "fYn_OJzPI4mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "ljS2xSeZb60b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        Scaled QK 값 구하기\n",
        "        \"\"\"\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "        QK = tf.matmul(Q, K, transpose_b = True)\n",
        "\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "       \n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9) \n",
        "\n",
        "        \"\"\"\n",
        "        1. Attention Weights 값 구하기 -> attentions\n",
        "        2. Attention 값을 V에 곱하기 -> out\n",
        "        \"\"\" \n",
        "        attentions = tf.nn.softmax(scaled_qk,axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "\n",
        "        return out, attentions\n",
        "        \n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Embedding을 Head의 수로 분할하는 함수\n",
        "\n",
        "        x: [ batch x length x emb ]\n",
        "        return: [ batch x length x heads x self.depth ]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        split_x = tf.reshape(x,(batch_size,-1,self.num_heads,self.depth))\n",
        "        split_x = tf.transpose(split_x,perm=[0,2,1,3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"\n",
        "        분할된 Embedding을 하나로 결합하는 함수\n",
        "\n",
        "        x: [ batch x length x heads x self.depth ]\n",
        "        return: [ batch x length x emb ]\n",
        "        \"\"\"\n",
        "        batch_size = x.shpae[0]\n",
        "        combined_x = tf.transpose(x, perm=[0,2,1,3])\n",
        "        combined_x = tf.reshape(combined_x,(batch,-1,self.d_model))\n",
        "        return combined_x\n",
        "    \n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        아래 순서에 따라 소스를 작성하세요.\n",
        "\n",
        "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
        "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
        "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
        "                 -> out, attention_weights\n",
        "        Step 4: Combine Heads(out) -> out\n",
        "        Step 5: Linear_out(out) -> out\n",
        "\n",
        "        \"\"\"\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "\n",
        "        WQ_splits = self.spilit_heads(WQ)\n",
        "        WK_splits = self.spilit_heads(WK)\n",
        "        WV_splits = self.spilit_heads(WV)\n",
        "\n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits,mask)\n",
        "        \n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out, attention_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "a5KQC6jpI-Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position-wise Feed-Forward Network"
      ],
      "metadata": {
        "id": "Oa6Je7q6ggG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.w_1(x)\n",
        "        out = self.w_2(out)\n",
        "            \n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "MX7vStmugWyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "1FVeZoPZhLXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        return out, enc_attn\n",
        "\n"
      ],
      "metadata": {
        "id": "jQf6EQIJg8Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "S--rWNz_hwOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "    \n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Masked Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn"
      ],
      "metadata": {
        "id": "hUB4cydthsr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 여러개의 레이어 생성"
      ],
      "metadata": {
        "id": "ntInD-KriA7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                        for _ in range(n_layers)]\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "    \n",
        "        enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "        \n",
        "        return out, enc_attns\n",
        "\n"
      ],
      "metadata": {
        "id": "2QXX7ncnhxeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                            for _ in range(n_layers)]\n",
        "                            \n",
        "                            \n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        out = x\n",
        "    \n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = \\\n",
        "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "\n",
        "        return out, dec_attns, dec_enc_attns\n",
        "\n"
      ],
      "metadata": {
        "id": "trEG2IGaiDiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 최종 Transformer"
      ],
      "metadata": {
        "id": "IlUwxC-SiVhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    src_vocab_size,\n",
        "                    tgt_vocab_size,\n",
        "                    pos_len,\n",
        "                    dropout=0.2,\n",
        "                    shared=True):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared = shared\n",
        "\n",
        "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "        seq_len = x.shape[1]\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "        \n",
        "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "        \n",
        "        dec_out, dec_attns, dec_enc_attns = \\\n",
        "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
        "        \n",
        "        logits = self.fc(dec_out)\n",
        "        \n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "L09Hn-O3iGuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking"
      ],
      "metadata": {
        "id": "6dYXX5g7iyJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def generate_causality_mask(src_len, tgt_len):\n",
        "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
        "    return tf.cast(mask, tf.float32)\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_mask = generate_padding_mask(tgt)\n",
        "\n",
        "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
        "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
        "\n",
        "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
        "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "ypTJes73iXdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 가변적인 Learning Rate"
      ],
      "metadata": {
        "id": "nh-0fu3Qi7Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = step ** -0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = LearningRateScheduler(512)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n"
      ],
      "metadata": {
        "id": "irOGg-RYizsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer로 번역기 만들기"
      ],
      "metadata": {
        "id": "bbM254uFxWoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 다운로드 \n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'train.zip',\n",
        "    origin='https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz',\n",
        "    extract=True)\n",
        "\n",
        "kor_path = os.path.dirname(path_to_zip)+\"/korean-english-park.train.ko\"\n",
        "eng_path = os.path.dirname(path_to_zip)+\"/korean-english-park.train.en\"\n"
      ],
      "metadata": {
        "id": "s2SdKsmgi-dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_corpus(kor_path, eng_path):\n",
        "    with open(kor_path, \"r\") as f: \n",
        "        kor = f.read().splitlines()\n",
        "    with open(eng_path, \"r\") as f: \n",
        "        eng = f.read().splitlines()\n",
        "    assert len(kor) == len(eng)\n",
        "\n",
        "    raw = zip(kor, eng)\n",
        "    cleaned_corpus = set(raw)\n",
        "\n",
        "    return cleaned_corpus\n",
        "\n",
        "cleaned_corpus = clean_corpus(kor_path, eng_path)"
      ],
      "metadata": {
        "id": "kSa57muPxZaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kor_corpus, eng_corpus = zip(*cleaned_corpus)\n",
        "print(len(kor_corpus), len(eng_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiPYavrXx0t_",
        "outputId": "cf559bf6-7151-4419-d745-c39171ad4642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78968 78968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 전처리\n",
        "- 모든 입력을 소문자로 변환합니다.\n",
        "- 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
        "-문장부호 양옆에 공백을 추가합니다.\n",
        "\n",
        "- 문장 앞뒤의 불필요한 공백을 제거합니다."
      ],
      "metadata": {
        "id": "H7Q9DbEox5oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#전처리 \n",
        "\n",
        "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
        "    sentence = sentence.lower().strip()\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!가-힣ㄱ-ㅎㅏ-ㅣ]+\", \" \", sentence)\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    \n",
        "    return sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "e1RF4o90x2fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_corpus = []\n",
        "dec_corpus = []\n",
        "\n",
        "for kor, eng in zip(kor_corpus, eng_corpus):\n",
        "    temp_kor = preprocess_sentence(kor)\n",
        "    temp_eng = preprocess_sentence(eng)\n",
        "\n",
        "    enc_corpus.append(temp_kor)\n",
        "    dec_corpus.append(temp_eng)\n",
        "    \n",
        "print('korean data size:', len(enc_corpus))\n",
        "print('english data size:', len(dec_corpus))\n",
        "print(\"Korean:\", enc_corpus[10])   \n",
        "print(\"English:\", dec_corpus[10])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrJEQvVQyawk",
        "outputId": "5050c5ff-2008-485c-d987-61130d787da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "korean data size: 78968\n",
            "english data size: 78968\n",
            "Korean: 클린턴은 모든 의견이 정책이 될 수는 없더라도 그녀의 귀와 생각은 열려 있을 것이라고 덧붙였습니다 .\n",
            "English: clinton adds that not everybody s ideas will become policy but indicates her ears and mind are open .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q10z6NZnRua9",
        "outputId": "a6b58814-3189-4f2e-8293-328387718302"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZiiFTPmTx4f",
        "outputId": "628ead24-ff6a-466f-b6a5-f641a44e5bef"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpSczAxOT3A1",
        "outputId": "185157cb-5803-4951-83da-057a83b0d531"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 토큰화"
      ],
      "metadata": {
        "id": "1eTKrfcdyx2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "\n",
        "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
        "def generate_tokenizer(corpus,\n",
        "                        vocab_size,\n",
        "                        lang=\"ko\",\n",
        "                        pad_id=0,\n",
        "                        bos_id=1,\n",
        "                        eos_id=2,\n",
        "                        unk_id=3):\n",
        "    \n",
        "    model_name= 'spm_'+lang\n",
        "    \n",
        " \n",
        "    ##모델 생성 경로\n",
        "    temp_file = '/content/drive/MyDrive/'+lang+'.ko'\n",
        "\n",
        "\n",
        "    with open(temp_file, 'w') as f:\n",
        "        for row in corpus:   # lang corpus를 활용합니다.\n",
        "            f.write(str(row) + '\\n')\n",
        "    \n",
        "    spm.SentencePieceTrainer.Train(\n",
        "         '\\\n",
        "        --input={} \\\n",
        "        --model_prefix={} \\\n",
        "        --vocab_size={} \\\n",
        "        --pad_id={} \\\n",
        "        --bos_id={} \\\n",
        "        --eos_id={} \\\n",
        "        --unk_id={}'.format(temp_file, model_name, vocab_size, pad_id, bos_id, eos_id, unk_id)    \n",
        "    )\n",
        "    \n",
        "    \n",
        "    tokenizer = spm.SentencePieceProcessor()\n",
        "    tokenizer.Load(model_name+'.model')\n",
        "    \n",
        "\n",
        "    return tokenizer\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "9q1893ewyvyL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 10000\n",
        "\n",
        "\n",
        "ko_tokenizer = generate_tokenizer(enc_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
        "en_tokenizer = generate_tokenizer(dec_corpus, TGT_VOCAB_SIZE, \"en\")\n",
        "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
      ],
      "metadata": {
        "id": "a8J65u1EypZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc32ed1d-afcb-4ce6-941c-11a02167f4d9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_corpus[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uIymms4CR4fh",
        "outputId": "a267a360-915e-4049-f078-5acd307f2669"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'휴가를 즐기는 행락객들이 만리장성의 흉벽에 쓰레기를 여기 저기 버리고 만리장성의 고대 벽돌에 자신들의 이름 머리글자들을 새겨 넣고 있다 .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm    # Process 과정을 보기 위해\n",
        "\n",
        "src_corpus = []\n",
        "tgt_corpus = []\n",
        "\n",
        "assert len(enc_corpus) == len(enc_corpus)\n",
        "\n",
        "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
        "for idx in tqdm(range(len(enc_corpus))):\n",
        "    token_ko = ko_tokenizer.encode_as_ids(enc_corpus[idx])\n",
        "    token_en = en_tokenizer.encode_as_ids(dec_corpus[idx])\n",
        "       \n",
        "    if len(token_ko) <= 50  and len(token_en) <= 50:\n",
        "        src_corpus.append(token_ko)\n",
        "        tgt_corpus.append(token_en)\n",
        "    \n",
        "\n",
        "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
        "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
        "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8d0e231d2558460b87e2b216be8d0407",
            "18696c9ff70149f9967fe591f8adf8bd",
            "97a09b05ac354b3683786234883c8851",
            "7420dcf1374f4f7b9b789a7cc79383a9",
            "313bb82aff944914beb94723e3710ddd",
            "496194799097486ba212fd4e22addaa2",
            "0699a8e0144e41e081a1d6a0cf558e75",
            "3b7cd9deb21e41af895901b516b655b4",
            "66611682c8e247bcb2385434058b151a",
            "69b4897748df46ebb0b736d4d594a944",
            "9dceb35ec7e34779a4975deddfba5fcc"
          ]
        },
        "id": "Z4r2PyqmWOkM",
        "outputId": "11b3023b-b610-41ad-83ff-c71034f66163"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/78968 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d0e231d2558460b87e2b216be8d0407"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(enc_train.shape)\n",
        "print(dec_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaglJUYVW-G3",
        "outputId": "608a328d-de8e-4df7-e746-a0a88ac05259"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(71771, 50)\n",
            "(71771, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련하기"
      ],
      "metadata": {
        "id": "IP_Znex9X6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    n_layers = 2,\n",
        "    d_model = 512,\n",
        "    n_heads = 8,\n",
        "    d_ff = 2048,\n",
        "    src_vocab_size=6000,\n",
        "    tgt_vocab_size=6000,\n",
        "    pos_len=50,\n",
        "    dropout = 0.3\n",
        ")"
      ],
      "metadata": {
        "id": "TW1vDCdfXxmN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learning_rate =0.0005\n",
        "learning_rate = LearningRateScheduler(512)\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-09, \n",
        "    name='Adam'\n",
        ")"
      ],
      "metadata": {
        "id": "-K5NG8_wX9eQ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "gCfv-4FiX-zl"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Step 함수 정의\n",
        "\n",
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    gold = tgt[:, 1:]\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
        "\n",
        "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
        "        loss = loss_function(gold, predictions[:, :-1])\n",
        "\n",
        "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다.\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "812DYCr4YAJh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T5LIXgoeYBwy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}