{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968e8253",
   "metadata": {},
   "source": [
    "## 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e604f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os,re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708495dc",
   "metadata": {},
   "source": [
    "## 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c3285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 패스\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 패스\n",
    "\n",
    "    if idx > 9: break   # 문장 10개 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d273f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    #소문자로 바꾸고 양쪽 공백 삭제\n",
    "    sentence = sentence.lower().strip()   \n",
    "    # 특수문자 양쪽에 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    # 여러 공백을 하나의 공백으로 바꿈\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    # a-zA-Z?.!,¿ 을 제외한 모든 문자를 하나의 공백으로 바꿈\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    # 양쪽 공백 삭제\n",
    "    sentence = sentence.strip()\n",
    "    # 문장 시작에는 <start>, 문장 끝에는 <end>를 추가\n",
    "    sentence = '<start> ' + sentence + ' <end>'      \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4673ef56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모음\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 이전처럼 우리가 원하지 않는 문장은 pass\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 정제를 list에 추가\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    #길이 15이상인 토큰 제외\n",
    "    if len(preprocessed_sentence.split()) > 15: continue\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50e86c",
   "metadata": {},
   "source": [
    "## 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ba04e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f7f9b894340>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    \n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000, # 15000단어를 기억할 수 있는 tokenizer 생성\n",
    "        filters=' ',    # 우리는 필터를 사용하지 않아도 됨\n",
    "        oov_token=\"<unk>\" # 15000단어에 포함되지 못한 단어는 '<unk>'로!\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 사전 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19161aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 토큰화된 단어 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c2ff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dce44a",
   "metadata": {},
   "source": [
    "## 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bcdc829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          train_size = 0.8,\n",
    "                                                          random_state= 123)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09482437",
   "metadata": {},
   "source": [
    "## 데이터셋 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd5cd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 15000개와, 여기 포함되지 않은 0:<pad>를 포함하여 15001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset1 = dataset1.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset1)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "dataset2 = dataset2.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013995d3",
   "metadata": {},
   "source": [
    "## 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3d9a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.Model 상속해서 사용\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1024\n",
    "hidden_size = 2000\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2da3aeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 15001), dtype=float32, numpy=\n",
       "array([[[-6.43394160e-05, -1.80767660e-04, -4.10567518e-05, ...,\n",
       "         -3.58101752e-05,  3.79382080e-04,  7.90397098e-05],\n",
       "        [-2.71101540e-04, -1.78351562e-04, -3.25585366e-04, ...,\n",
       "         -2.93844554e-04,  5.37892687e-04,  3.02417408e-04],\n",
       "        [-1.49225583e-04,  4.89592145e-04, -2.47398653e-04, ...,\n",
       "         -5.75735059e-04,  4.96912806e-04,  7.21081509e-04],\n",
       "        ...,\n",
       "        [ 1.00437237e-03,  1.36546558e-03, -1.09741150e-03, ...,\n",
       "          1.02055410e-03, -1.80016831e-03,  3.92881018e-04],\n",
       "        [ 1.07844942e-03,  1.68935815e-03, -1.69803039e-03, ...,\n",
       "          1.42133562e-03, -2.02901615e-03,  5.05986856e-04],\n",
       "        [ 1.12037500e-03,  2.00962787e-03, -2.21423968e-03, ...,\n",
       "          1.72396435e-03, -2.22957600e-03,  6.75101066e-04]],\n",
       "\n",
       "       [[-6.43394160e-05, -1.80767660e-04, -4.10567518e-05, ...,\n",
       "         -3.58101752e-05,  3.79382080e-04,  7.90397098e-05],\n",
       "        [-1.39270953e-04, -3.73566087e-04, -2.05671458e-04, ...,\n",
       "         -2.82125256e-04,  4.73272463e-04, -1.53913250e-04],\n",
       "        [-7.38351009e-05, -5.48717217e-04, -2.73604383e-04, ...,\n",
       "         -6.79171411e-04,  7.17591261e-04, -1.99667164e-04],\n",
       "        ...,\n",
       "        [-2.37661717e-03,  3.49922891e-04,  1.77929515e-03, ...,\n",
       "          5.28098084e-04,  9.14497796e-05,  1.57066283e-03],\n",
       "        [-2.13079480e-03,  1.52406676e-04,  1.24059897e-03, ...,\n",
       "          1.17258495e-03, -1.82178075e-04,  1.28891366e-03],\n",
       "        [-1.73890416e-03,  1.46378268e-04,  4.72462736e-04, ...,\n",
       "          1.76161213e-03, -4.78592236e-04,  1.04586605e-03]],\n",
       "\n",
       "       [[-6.43394160e-05, -1.80767660e-04, -4.10567518e-05, ...,\n",
       "         -3.58101752e-05,  3.79382080e-04,  7.90397098e-05],\n",
       "        [-4.17905336e-04,  1.06303254e-04, -2.97505874e-04, ...,\n",
       "          4.95283894e-05,  2.65587354e-04,  8.18290049e-04],\n",
       "        [-7.46104168e-04,  3.41715291e-04, -6.91335415e-04, ...,\n",
       "         -1.07880405e-04,  1.95119079e-04,  1.27113343e-03],\n",
       "        ...,\n",
       "        [-7.11070490e-04,  5.42651047e-04,  4.40827971e-05, ...,\n",
       "          2.96461745e-03, -2.43450212e-03,  5.36496053e-04],\n",
       "        [-3.21487023e-04,  8.21714930e-04, -5.31960279e-04, ...,\n",
       "          3.11488146e-03, -2.76963972e-03,  4.89137601e-04],\n",
       "        [ 1.19259721e-05,  1.17418845e-03, -1.10678968e-03, ...,\n",
       "          3.18931206e-03, -3.03309271e-03,  5.21410082e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.43394160e-05, -1.80767660e-04, -4.10567518e-05, ...,\n",
       "         -3.58101752e-05,  3.79382080e-04,  7.90397098e-05],\n",
       "        [ 9.07561116e-05, -4.03608050e-04, -2.59092514e-04, ...,\n",
       "          5.91911448e-05,  3.52977106e-04,  4.25304839e-04],\n",
       "        [-2.59525899e-04, -3.48461588e-04, -2.31857062e-04, ...,\n",
       "          3.69025191e-04,  9.74085378e-06, -1.29290973e-04],\n",
       "        ...,\n",
       "        [-2.30560501e-04,  1.34269521e-03, -2.35021161e-03, ...,\n",
       "          2.46081757e-03, -2.38206470e-03,  1.34483533e-04],\n",
       "        [ 3.12841803e-05,  1.74046843e-03, -2.77146162e-03, ...,\n",
       "          2.56343698e-03, -2.43678177e-03,  4.21861594e-04],\n",
       "        [ 2.40520734e-04,  2.09352490e-03, -3.11440020e-03, ...,\n",
       "          2.60780728e-03, -2.48711160e-03,  7.20064680e-04]],\n",
       "\n",
       "       [[-6.43394160e-05, -1.80767660e-04, -4.10567518e-05, ...,\n",
       "         -3.58101752e-05,  3.79382080e-04,  7.90397098e-05],\n",
       "        [-4.07568703e-04, -3.23574146e-04,  5.08535122e-05, ...,\n",
       "          2.74809776e-04,  2.15084699e-04,  2.67777767e-04],\n",
       "        [-1.02443562e-03, -4.82903648e-04, -1.74159184e-04, ...,\n",
       "          3.16660997e-04,  2.01769471e-05,  3.49187059e-04],\n",
       "        ...,\n",
       "        [-1.67126406e-03,  4.05955245e-04,  4.26526472e-04, ...,\n",
       "          2.10723071e-03, -5.53889491e-04, -2.15941877e-03],\n",
       "        [-1.41107710e-03,  5.01447299e-04,  1.76599187e-05, ...,\n",
       "          2.35661957e-03, -9.18441918e-04, -1.90174743e-03],\n",
       "        [-1.08439219e-03,  7.24506157e-04, -5.25897194e-04, ...,\n",
       "          2.56534084e-03, -1.25104235e-03, -1.59073586e-03]],\n",
       "\n",
       "       [[-6.43394160e-05, -1.80767660e-04, -4.10567518e-05, ...,\n",
       "         -3.58101752e-05,  3.79382080e-04,  7.90397098e-05],\n",
       "        [-4.07568703e-04, -3.23574146e-04,  5.08535122e-05, ...,\n",
       "          2.74809776e-04,  2.15084699e-04,  2.67777767e-04],\n",
       "        [-8.26556294e-04, -2.76820501e-04,  2.54353217e-04, ...,\n",
       "          9.09543829e-04, -4.48100109e-05, -8.38491032e-05],\n",
       "        ...,\n",
       "        [-9.21596133e-04,  7.65856297e-04,  2.95701757e-04, ...,\n",
       "          1.84954668e-04, -3.09212104e-04,  7.51578307e-04],\n",
       "        [-8.79501051e-04,  7.06615741e-04,  7.44714489e-05, ...,\n",
       "          5.90018055e-04, -6.73492905e-04,  3.59895464e-04],\n",
       "        [-7.45406665e-04,  7.65243720e-04, -4.20370547e-04, ...,\n",
       "          1.05186878e-03, -1.07799366e-03,  5.68803916e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  model에 데이터 한 배치만 태워보기\n",
    "for src_sample, tgt_sample in dataset1.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc2ff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  15361024  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  24200000  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  32008000  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  30017001  \n",
      "=================================================================\n",
      "Total params: 101,586,025\n",
      "Trainable params: 101,586,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14cf70a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 288s 587ms/step - loss: 3.2753 - val_loss: 2.8998\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 286s 588ms/step - loss: 2.7119 - val_loss: 2.6394\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 286s 587ms/step - loss: 2.3677 - val_loss: 2.4479\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 286s 588ms/step - loss: 2.0348 - val_loss: 2.3142\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 287s 589ms/step - loss: 1.7291 - val_loss: 2.2187\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 287s 590ms/step - loss: 1.4688 - val_loss: 2.1661\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 287s 589ms/step - loss: 1.2651 - val_loss: 2.1482\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 288s 590ms/step - loss: 1.1216 - val_loss: 2.1565\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 287s 590ms/step - loss: 1.0370 - val_loss: 2.1790\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 287s 590ms/step - loss: 0.9934 - val_loss: 2.2010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7efbf5e970>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습해보기\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset1, epochs=10,validation_data = dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f11f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9877513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love it when you call me big poppa <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5fdd3",
   "metadata": {},
   "source": [
    "## 회고\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa864e",
   "metadata": {},
   "source": [
    "이번 노드는 개인적으로 제일 어려웠던 노드였던것 같다.\n",
    "모델 만드는 과정도 내가 여태까지 해왔던 layer를 쌓는 방식과는 뭔가 달라서 이해하는게 매우 어려웠고,\n",
    "텍스트 데이터를 벡터화 시키는것도 이해가 아직도 잘 가지 않는 부분이 많다.\n",
    "\n",
    "아이펠 후반기에 cv와 nlp중 하나를 선택해야 한다면 당연히 nlp를 해야지 라고 생각했었는데,\n",
    "진짜 너무 어려운것 같다.. 아직은 노드가 초기 단계여서 더 어렵게 느껴지는것 같은데, 그만큼 nlp를 배워보고 싶다는 욕구도 더 커지는것 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3895a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
